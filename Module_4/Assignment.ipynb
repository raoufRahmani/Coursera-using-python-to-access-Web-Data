{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57354746",
   "metadata": {},
   "source": [
    " http://py4e-data.dr-chuck.net/comments_42.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba82dc1",
   "metadata": {},
   "source": [
    "This program connects to a web page, reads the HTML content, and looks for all the numbers inside `<span>` tags that have the class `\"comments\"`. It collects these numbers, counts how many there are, and calculates the total sum. This is a basic example of web scraping, where we extract useful information from a website automatically using Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c1d05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 50\n",
      "Sum  : 2440\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "url = input(\"Enter Url :\")\n",
    "html = urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "numbers = [int(span.text) for span in soup('span', class_='comments')]\n",
    "print('Count:', len(numbers))\n",
    "print('Sum  :', sum(numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbaa73b",
   "metadata": {},
   "source": [
    "##### Links :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b498c911",
   "metadata": {},
   "source": [
    "Train : http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)  \n",
    "Actual data for the assignment: http://py4e-data.dr-chuck.net/comments_2251561.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87cbef",
   "metadata": {},
   "source": [
    "Assignment 2 : Following Links in HTML Using BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504e4d9",
   "metadata": {},
   "source": [
    "This program will start from a web page whose URL you enter, download that page, and look at all the links (\"a\" tags) on it. It will then pick the link that sits in a specific position in that list (for example, the 18‑th link if the position is 18), follow that link to the next page, and repeat the same process a fixed number of times (for example, seven “hops”). At each hop it prints the URL it just followed so you can see the chain of pages. When the loop finishes, the last page you land on contains the final name you need to report for the assignment. In short, the script crawls forward through a sequence of pages, always choosing the link in the same numbered position, repeating this for the requested count of steps, and shows you where you end up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next URL: http://py4e-data.dr-chuck.net/known_by_Carla.html\n",
      "Next URL: http://py4e-data.dr-chuck.net/known_by_Isobella.html\n",
      "Next URL: http://py4e-data.dr-chuck.net/known_by_Clarizze.html\n",
      "Next URL: http://py4e-data.dr-chuck.net/known_by_Raonaid.html\n",
      "Next URL: http://py4e-data.dr-chuck.net/known_by_Lulu.html\n",
      "Next URL: http://py4e-data.dr-chuck.net/known_by_Marley.html\n",
      "Next URL: http://py4e-data.dr-chuck.net/known_by_Campbel.html\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "ctx==ssl.create_default_context()\n",
    "ctx.check_hostname=False\n",
    "ctx.verify_mode=ssl.CERT_NONE\n",
    "\n",
    "url=input('Enter Url')\n",
    "html=urllib.request.urlopen(url, context=ctx) \n",
    "soup=BeautifulSoup(html,\"html.parser\")\n",
    "tags=soup('a')\n",
    "for tag in tags :\n",
    "    liste=[]\n",
    "    x=tag.get('href',None) \n",
    "    liste.append(x)     \n",
    "for _ in range(7):\n",
    "    # download the untcurrent page\n",
    "    html  = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup  = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # grab all the <a> tags on that page *once*\n",
    "    links = soup('a')                \n",
    "    url   = links[17]['href']  \n",
    "    print('Next URL:', url)          \n",
    "\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c119f19",
   "metadata": {},
   "source": [
    "Actual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Tess.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097d5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
